{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def constructLabeledSentences(data):\n",
    "    sentences = []\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def getEmbeddings(path,vector_dimension=300):\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
    "\n",
    "    x = constructLabeledSentences(data['text'])\n",
    "    y = data['label'].values\n",
    "\n",
    "    text_model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10,\n",
    "                         seed=1)\n",
    "    text_model.build_vocab(x)\n",
    "    text_model.train(x, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
    "\n",
    "    train_size = int(0.8 * len(x))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    text_train_arrays = np.zeros((train_size, vector_dimension))\n",
    "    text_test_arrays = np.zeros((test_size, vector_dimension))\n",
    "    train_labels = np.zeros(train_size)\n",
    "    test_labels = np.zeros(test_size)\n",
    "\n",
    "    for i in range(train_size):\n",
    "        text_train_arrays[i] = text_model.docvecs['Text_' + str(i)]\n",
    "        train_labels[i] = y[i]\n",
    "\n",
    "    j = 0\n",
    "    for i in range(train_size, train_size + test_size):\n",
    "        text_test_arrays[j] = text_model.docvecs['Text_' + str(i)]\n",
    "        test_labels[j] = y[i]\n",
    "        j = j + 1\n",
    "\n",
    "    return text_train_arrays, text_test_arrays, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
